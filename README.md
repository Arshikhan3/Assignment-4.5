# Assignment-4.5
q1.  Explain what is checksum and the importance of checksum and how hadoop performs checksum?
Ans: When we download files from certain websites, they have a very long string of numbers and letters called a checksum. These really long strings basically act as fingerprints for that particular file, whether it be an EXE, ISO, ZIP, etc.

Checksums are used to ensure the integrity of a file after it has been transmitted from one storage device to another. This can be across the Internet or simply between two computers on the same network. Either way, if we want to ensure that the transmitted file is exactly the same as the source file, we can use a checksum.

The checksum is calculated using a hash function and is normally posted along with the download. To verify the integrity of the file, a user calculates the checksum using a checksum calculator program and then compares the two to make sure they match.

Checksums are used not only to ensure a corrupt-free transmission, but also to ensure that the file has not been tampered with. When a good checksum algorithm is used, even a tiny change to the file will result in a completely different checksum value.

The most common checksums are MD5 and SHA-1, but both have been found to have vulnerabilities. This means that malicious tampering can lead to two different files having the same computed hash. Due to these security concerns, the newer SHA-2 is considered the best cryptographic hash function since no attack has been demonstrated on it as of yet.

q.2. Explain the anatomy of file write to HDFS?
Ans: Name node saves part of HDFS metadata like file location, permission, etc. in files called namespace image and edit logs. Files are stored in HDFS as blocks. These block information are not saved in any file. Instead it is gathered every time the cluster is started. And this information is stored in name node’s memory. 2. Replica Placement : Assuming the replication factor is 3; When a file is written from a data node (say R1N1), Hadoop attempts to save the first replica in same data node (R1N1). Second replica is written into another node (R2N2) in a different rack (R2). Third replica is written into another node (R2N1) in the same rack (R2) where the second replica was saved. 3. Hadoop takes a simple approach in which the network is represented as a tree and the distance between two nodes is the sum of their distances to their closest common ancestor. The levels can be like; “Data Center” > “Rack” > “Node”. Example; ‘/d1/r1/n1’ is a representation for a node named n1 on rack r1 in data center d1. Distance calculation has 4 possible scenarios as; 1. distance(/d1/r1/n1, /d1/r1/n1) = 0 [Processes on same node] 2. distance(/d1/r1/n1, /d1/r1/n2) = 2 [different node is same rack] 3. distance(/d1/r1/n1, /d1/r2/n3) = 4 [node in different rack]

q.3. Explain how HDFS handles failures during file write?
Ans: While writing the blocks in the data nodes one of the writing to the data node fails. Immediately the pipeline will be closed and the data nodes which are working fine are given a new identity. This change is conveyed to Name node. Once this is conveyed the failed data node is having partial data of the block that data is removed because it may cause confusion if the data node gets repaired and come back live. The remaining blocks in the queue are moved to the data queue and they are written into the two data nodes(considered replication factor as 3)once done the hdfs recognizes that block has been under replicated and hence generates another cop of the block in some fully functional data node.
